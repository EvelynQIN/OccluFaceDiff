save_dir: "./checkpoints/e2e" 
num_epoch: 300
dataset: "FaMoS" 
dataset_path: "./processed_data"
weight_decay: 0.01
batch_size: 1 
gradient_accumulation_steps: 256
lr: 0.0003 
cosine_scheduler: True
device: 0
num_workers: 8 
diffusion_steps: 1000 
overwrite: True 
no_normalization: False
train_dataset_repeat_times: 256
# resume_checkpoint: "checkpoints/Transformer_128d_nomask_1l/model_140.pt"

# log
wandb_log: True
save_interval: 2 
log_interval: 1 

#model architecture
arch: "diffusion_Transformer_68_1024d_8h" 
target_nfeat: 433
lmk3d_dim: 204  # 68x3
lmk2d_dim: 136  # 68x2

latent_dim: 1024
dropout: 0.1
use_mask: True  # use alibi mask

# unusable for gru
ff_size: 2048
num_enc_layers: 2
num_heads: 8

# unusable for transformer encoder 
num_dec_layers: 4 

# unusable for warmup cosine scheduler
# lr_anneal_steps: 225000 

# for flame
flame_model_path: "flame_2020/generic_model.pkl"
flame_lmk_embedding_path: "flame_2020/dense_lmk_embedding.npy"