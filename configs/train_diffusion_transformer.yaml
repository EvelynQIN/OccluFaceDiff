save_dir: "./checkpoints" 
num_epoch: 30
dataset: "multiface" 
dataset_path: "./dataset"
weight_decay: 0.001
batch_size: 1 
gradient_accumulation_steps: 32
lr: 0.0003 
cosine_scheduler: True
device: 0
num_workers: 8 
diffusion_steps: 1000 
overwrite: True 
no_normalization: False
train_dataset_repeat_times: 10
occlusion_mask_prb: 0.5
mixed_occlusion_prob: 0.3
fps: 30
# resume_checkpoint: "checkpoints/Transformer_128d_nomask_1l/model_140.pt"

# flame_params
n_exp: 50 
n_pose: 6 # jaw pose in 6d


# log
wandb_log: True
save_interval: 1 
log_interval: 1 

#model architecture
arch: "diffusion_Transformer_256d_1l_4h" 
latent_dim: 256
dropout: 0.2

use_mask: False  # use alibi mask

# unusable for gru
ff_size: 512
num_enc_layers: 1
num_heads: 4

# unusable for transformer encoder 
num_dec_layers: 1

# for flame
# flame_model_path: "flame_2020/generic_model.pkl"
# flame_lmk_embedding_path: "flame_2020/dense_lmk_embedding.npy"

# training losses
shape_loss_w: 0.01
pose_loss_w: 5.0
expr_loss_w: 5.0
trans_loss_w: 5.0
mouth_closure_loss_w: 1.0
eye_closure_loss_w: 1.0
verts3d_loss_w: 1000.0
lmk2d_loss_w: 0.1
verts2d_loss_w: 100.0
pose_jitter_w: 0.1
exp_jitter_w: 0.1