save_dir: "./checkpoints/e2e1" 
num_epoch: 30
dataset: "FaMoS" 
dataset_path: "./processed_data"
weight_decay: 0.01
batch_size: 1 
gradient_accumulation_steps: 128
lr: 0.0003 
cosine_scheduler: True
device: 0
num_workers: 8 
diffusion_steps: 1000 
overwrite: True 
no_normalization: False
train_dataset_repeat_times: 128
occlusion_mask_prb: 0.5
mixed_occlusion_prob: 0.3
fps: 30
# resume_checkpoint: "checkpoints/Transformer_128d_nomask_1l/model_140.pt"

# flame_params
n_shape: 100
n_exp: 50 
n_pose: 30 
n_trans: 3

# log
wandb_log: True
save_interval: 1 
log_interval: 1 

#model architecture
arch: "diffusion_Transformer_68_256d_1l_4h_occ" 
target_nfeat: 183
lmk3d_dim: 204  # 68x3
lmk2d_dim: 136  # 68x2

latent_dim: 256
dropout: 0.1
use_mask: True  # use alibi mask

# unusable for gru
ff_size: 512
num_enc_layers: 1
num_heads: 4

# unusable for transformer encoder 
num_dec_layers: 1

# unusable for warmup cosine scheduler
# lr_anneal_steps: 225000 

# for flame
flame_model_path: "flame_2020/generic_model.pkl"
flame_lmk_embedding_path: "flame_2020/dense_lmk_embedding.npy"