{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from transformers import Wav2Vec2Processor\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4d2f9e342b4627ba5f78c86da8d1fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/212 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db51760b26f240d488c5dda8a18b099a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0ff414bfe24dc4ab42b9df15cacb98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c729ebc4a1ec456db82caeb74809bd0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/292 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a867780e414b789087e26c679a6066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "audio_processor = Wav2Vec2Processor.from_pretrained(\n",
    "        \"facebook/hubert-xlarge-ls960-ft\")  # HuBERT uses the processor of Wav2Vec 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49600,)\n",
      "secs: 3.1\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "audio_path = 'dataset/multiface/m--20171024--0000--002757580--GHS/audio/SEN_are_you_looking_for_employment.wav'\n",
    "speech_array, sampling_rate = librosa.load(audio_path, sr=16000)\n",
    "audio_values = np.squeeze(audio_processor(speech_array, return_tensors=None, padding=\"longest\",\n",
    "                            sampling_rate=sampling_rate).input_values)\n",
    "print(audio_values.shape)\n",
    "print(f\"secs: {audio_values.shape[0] / sampling_rate}\")\n",
    "print(f\"{type(audio_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check correspondances of MEAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import cv2 \n",
    "import os \n",
    "import glob\n",
    "import pickle\n",
    "import h5py\n",
    "from skimage.transform import estimate_transform, warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 1920, 3)\n"
     ]
    }
   ],
   "source": [
    "## test mediapipe original landmarks on the original images\n",
    "img_path = 'dataset/mead_25fps/processed/images/M003/front/angry/level_1/000001.png'\n",
    "img = cv2.imread(img_path)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83, 478, 3)\n",
      "(83, 478, 2)\n"
     ]
    }
   ],
   "source": [
    "lmk_aligned_path = 'dataset/mead_25fps/processed/landmarks_original/mediapipe/M003/front/angry/level_1/001'\n",
    "with open(os.path.join(lmk_aligned_path, 'landmarks_original.pkl'), 'rb') as f:\n",
    "    lmks_original = pickle.load(f)\n",
    "lmks_original_478 = np.asarray(lmks_original).squeeze(1)\n",
    "print(lmks_original_478.shape)\n",
    "\n",
    "with open(os.path.join(lmk_aligned_path, 'landmarks.pkl'), 'rb') as f:\n",
    "    landmark_478 = pickle.load(f)\n",
    "landmark_478 = np.asarray(landmark_478).squeeze(1)\n",
    "print(landmark_478.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point2bbox(center, size):\n",
    "    size2 = size / 2\n",
    "\n",
    "    src_pts = np.array(\n",
    "        [[center[0] - size2, center[1] - size2], [center[0] - size2, center[1] + size2],\n",
    "         [center[0] + size2, center[1] - size2]])\n",
    "    return src_pts\n",
    "\n",
    "def point2transform(center, size, target_size_height, target_size_width):\n",
    "    src_pts = point2bbox(center, size)\n",
    "    dst_pts = np.array([[0, 0], [0, target_size_width - 1], [target_size_height - 1, 0]])\n",
    "    tform = estimate_transform('similarity', src_pts, dst_pts)\n",
    "    return tform\n",
    "\n",
    "def warp_image_from_lmk(\n",
    "        landmarks, \n",
    "        img, \n",
    "        scale=1.35, \n",
    "        bb_center_shift_x=0., \n",
    "        bb_center_shift_y=-0.1,\n",
    "        image_size=224):    # defaults from EMOTE preprocessing script\n",
    "    left = np.min(landmarks[:, 0])\n",
    "    right = np.max(landmarks[:, 0])\n",
    "    top = np.min(landmarks[:, 1])\n",
    "    bottom = np.max(landmarks[:, 1])\n",
    "\n",
    "    old_size = (right - left + bottom - top) / 2 * 1.1\n",
    "    center_x = right - (right - left) / 2.0 \n",
    "    center_y = bottom - (bottom - top) / 2.0\n",
    "    center = np.array([center_x, center_y])\n",
    "\n",
    "    center[0] += abs(right-left)*bb_center_shift_x\n",
    "    center[1] += abs(bottom-top)*bb_center_shift_y\n",
    "\n",
    "    size = int(old_size * scale)\n",
    "\n",
    "    tform = point2transform(center, size, image_size, image_size)\n",
    "    output_shape = (image_size, image_size)\n",
    "    dst_image = warp(img, tform.inverse, output_shape=output_shape, order=3)\n",
    "    dst_landmarks = tform(landmarks[:, :2])\n",
    "\n",
    "    return dst_image, dst_landmarks\n",
    "\n",
    "def draw_lmk_on_image(lmks, img, out_path):\n",
    "    # draw landmarks on the image\n",
    "    h, w, c = img.shape\n",
    "    for px in lmks[:,:2]:\n",
    "        x, y = int(px[0]), int(px[1])\n",
    "        if 0 <= x < w and 0 <=y < h:\n",
    "            cv2.circle(img, (x, y), 1, (0, 0, 255), 1)\n",
    "    cv2.imwrite(out_path, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(img_path)\n",
    "dst_image, dst_landmarks = warp_image_from_lmk(lmks_original_478[0], img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check warped landmarks equal to gt\n",
    "np.allclose(dst_landmarks, landmark_478[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_warped = (dst_image * 255).astype(np.uint8)\n",
    "draw_lmk_on_image(landmark_478[0], img_warped.copy(), \"test_mediapipe_aligned.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83, 68, 2)\n"
     ]
    }
   ],
   "source": [
    "# check FAN 68 landmarks\n",
    "lmk_path = 'dataset/mead_25fps/processed/landmarks_aligned/fan/M003/front/angry/level_1/001/landmarks.pkl'\n",
    "with open(lmk_path, 'rb') as f:\n",
    "    lmk_68 = pickle.load(f)\n",
    "print(lmk_68.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmk_68_224 = lmk_68[0].copy() * 224\n",
    "draw_lmk_on_image(lmk_68_224, img_warped.copy(), \"test_fan_aligned.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test audio input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "import librosa\n",
    "import numpy as np  \n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"/local/home/yaqqin/Downloads/013.m4a\" # \"dataset/mead_25fps/original_data/M003/audio/angry/level_1/001.m4a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.221375"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(speech_array) / sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_values = audio_processor(\n",
    "    speech_array, \n",
    "    return_tensors='pt', \n",
    "    padding=\"longest\",\n",
    "    sampling_rate=sampling_rate).input_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 51542])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.221375"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_values.shape[1] / sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.wav2vec import Wav2Vec2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27e846b36474802842b19e513212730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 83, 1024])\n"
     ]
    }
   ],
   "source": [
    "wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "wav2vec.feature_extractor._freeze_parameters()\n",
    "wav2vec.to('cuda')\n",
    " \n",
    "audio_input = audio_values.float().to('cuda')\n",
    "with torch.no_grad():\n",
    "    audio_emb = wav2vec(audio_input, frame_num = 83).last_hidden_state.cpu()\n",
    "    print(audio_emb.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
