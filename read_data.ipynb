{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from transformers import Wav2Vec2Processor\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4d2f9e342b4627ba5f78c86da8d1fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/212 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db51760b26f240d488c5dda8a18b099a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0ff414bfe24dc4ab42b9df15cacb98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c729ebc4a1ec456db82caeb74809bd0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/292 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a867780e414b789087e26c679a6066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "audio_processor = Wav2Vec2Processor.from_pretrained(\n",
    "        \"facebook/hubert-xlarge-ls960-ft\")  # HuBERT uses the processor of Wav2Vec 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49600,)\n",
      "secs: 3.1\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "audio_path = 'dataset/multiface/m--20171024--0000--002757580--GHS/audio/SEN_are_you_looking_for_employment.wav'\n",
    "speech_array, sampling_rate = librosa.load(audio_path, sr=16000)\n",
    "audio_values = np.squeeze(audio_processor(speech_array, return_tensors=None, padding=\"longest\",\n",
    "                            sampling_rate=sampling_rate).input_values)\n",
    "print(audio_values.shape)\n",
    "print(f\"secs: {audio_values.shape[0] / sampling_rate}\")\n",
    "print(f\"{type(audio_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read MEAD processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/mead_25fps/processed/metadata.pkl', 'rb') as f:\n",
    "    version = pickle.load(f)\n",
    "    video_list = pickle.load(f)\n",
    "    video_metas = pickle.load(f)\n",
    "    annotation_list = pickle.load(f)    # None\n",
    "    frame_lists = pickle.load(f)    # None\n",
    "    try:\n",
    "        audio_metas = pickle.load(f)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('M003/video/front/angry/level_1/001.mp4'),\n",
       " PosixPath('M003/video/front/angry/level_1/002.mp4')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'fps': '25/1',\n",
       "  'width': 1920,\n",
       "  'height': 1080,\n",
       "  'num_frames': 83,\n",
       "  'bit_rate': '7402260',\n",
       "  'bits_per_raw_sample': '8'},\n",
       " {'fps': '25/1',\n",
       "  'width': 1920,\n",
       "  'height': 1080,\n",
       "  'num_frames': 67,\n",
       "  'bit_rate': '7245600',\n",
       "  'bits_per_raw_sample': '8'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_metas[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of cam : (1, 83, 3)\n",
      "shape of exp : (1, 83, 100)\n",
      "shape of global_pose : (1, 83, 3)\n",
      "shape of jaw : (1, 83, 3)\n",
      "shape of shape : (1, 83, 300)\n"
     ]
    }
   ],
   "source": [
    "filename = 'dataset/mead_25fps/processed/reconstructions/EMICA-MEAD_flame2020/M003/front/angry/level_1/001/shape_pose_cam.hdf5'\n",
    "data_dict = {}\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    for k in f.keys():\n",
    "        print(f\"shape of {k} : {f[k].shape}\")\n",
    "        data_dict[k] = torch.from_numpy(f[k][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['exp'] = data_dict['exp'][...,:50]\n",
    "data_dict['pose'] = torch.cat([data_dict['global_pose'], data_dict['jaw']], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of images : (88, 3, 224, 224)\n",
      "shape of img_masks : (88, 224, 224)\n",
      "shape of lmk_2d : (88, 478, 2)\n",
      "shape of valid_frames_idx : (88,)\n"
     ]
    }
   ],
   "source": [
    "filename = 'cropped_frames.hdf5'\n",
    "data_dict = {}\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    for k in f.keys():\n",
    "        print(f\"shape of {k} : {f[k].shape}\")\n",
    "        data_dict[k] = f[k][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_lmk_on_image(lmks, img, out_path, color=(0,0,255)):\n",
    "    # draw landmarks on the image\n",
    "    h, w, c = img.shape\n",
    "    for px in lmks[:,:2]:\n",
    "        x, y = int(px[0]), int(px[1])\n",
    "        if 0 <= x < w and 0 <=y < h:\n",
    "            cv2.circle(img, (x, y), 1, color, 1)\n",
    "    cv2.imwrite(out_path, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check flame render correspondance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import get_cfg_defaults \n",
    "from utils.data_util import batch_orth_proj\n",
    "from model.FLAME import FLAME_mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg_defaults().model\n",
    "flame = FLAME_mediapipe(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EMICA_rec_to_lmk_2d(flame, frame_id, data_dict):\n",
    "    shape = data_dict['shape'][:,frame_id]\n",
    "    exp = data_dict['exp'][:,frame_id]\n",
    "    pose = data_dict['pose'][:,frame_id]\n",
    "    verts, lmk_3d = flame(shape, exp, pose)\n",
    "    print(lmk_3d.shape) \n",
    "\n",
    "    cam = data_dict['cam'][:,frame_id]\n",
    "    lmk2d_pred = batch_orth_proj(lmk_3d, cam)[:, :, :2]\n",
    "    lmk2d_pred[:, :, 1:] = -lmk2d_pred[:, :, 1:]\n",
    "    print(lmk2d_pred.shape)\n",
    "\n",
    "    lmk2d_pred = lmk2d_pred[0] * 112 + 112\n",
    "    return lmk2d_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check correspondances of MEAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import cv2 \n",
    "import os \n",
    "import glob\n",
    "import pickle\n",
    "import h5py\n",
    "from skimage.transform import estimate_transform, warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 1920, 3)\n"
     ]
    }
   ],
   "source": [
    "## test mediapipe original landmarks on the original images\n",
    "img_path = f'dataset/mead_25fps/processed/images/M003/front/angry/level_1/00000{frame_id+1}.png'\n",
    "img = cv2.imread(img_path)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83, 478, 3)\n",
      "(83, 478, 2)\n",
      "(83, 478, 2)\n"
     ]
    }
   ],
   "source": [
    "lmk_aligned_path = 'dataset/mead_25fps/processed/landmarks_original/mediapipe/M003/front/angry/level_1/001'\n",
    "with open(os.path.join(lmk_aligned_path, 'landmarks_original.pkl'), 'rb') as f:\n",
    "    lmks_original = pickle.load(f)\n",
    "lmks_original_478 = np.asarray(lmks_original).squeeze(1)\n",
    "print(lmks_original_478.shape)\n",
    "\n",
    "with open(os.path.join(lmk_aligned_path, 'landmarks.pkl'), 'rb') as f:\n",
    "    landmark_478 = pickle.load(f)\n",
    "landmark_478 = np.asarray(landmark_478).squeeze(1)\n",
    "print(landmark_478.shape)\n",
    "\n",
    "with open(os.path.join(lmk_aligned_path, 'landmarks_aligned_video_smoothed.pkl'), 'rb') as f:\n",
    "    landmark_478_smoothed = pickle.load(f)\n",
    "landmark_478_smoothed = np.asarray(landmark_478_smoothed)\n",
    "print(landmark_478_smoothed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point2bbox(center, size):\n",
    "    size2 = size / 2\n",
    "\n",
    "    src_pts = np.array(\n",
    "        [[center[0] - size2, center[1] - size2], [center[0] - size2, center[1] + size2],\n",
    "         [center[0] + size2, center[1] - size2]])\n",
    "    return src_pts\n",
    "\n",
    "def point2transform(center, size, target_size_height, target_size_width):\n",
    "    src_pts = point2bbox(center, size)\n",
    "    dst_pts = np.array([[0, 0], [0, target_size_width - 1], [target_size_height - 1, 0]])\n",
    "    tform = estimate_transform('similarity', src_pts, dst_pts)\n",
    "    return tform\n",
    "\n",
    "def warp_image_from_lmk(\n",
    "        landmarks, \n",
    "        img, \n",
    "        scale=1.35, \n",
    "        bb_center_shift_x=0., \n",
    "        bb_center_shift_y=-0.1,\n",
    "        image_size=224):    # defaults from EMOTE preprocessing script\n",
    "    left = np.min(landmarks[:, 0])\n",
    "    right = np.max(landmarks[:, 0])\n",
    "    top = np.min(landmarks[:, 1])\n",
    "    bottom = np.max(landmarks[:, 1])\n",
    "\n",
    "    old_size = (right - left + bottom - top) / 2 * 1.1\n",
    "    center_x = right - (right - left) / 2.0 \n",
    "    center_y = bottom - (bottom - top) / 2.0\n",
    "    center = np.array([center_x, center_y])\n",
    "\n",
    "    center[0] += abs(right-left)*bb_center_shift_x\n",
    "    center[1] += abs(bottom-top)*bb_center_shift_y\n",
    "\n",
    "    size = int(old_size * scale)\n",
    "\n",
    "    tform = point2transform(center, size, image_size, image_size)\n",
    "    output_shape = (image_size, image_size)\n",
    "    dst_image = warp(img, tform.inverse, output_shape=output_shape, order=3)\n",
    "    dst_landmarks = tform(landmarks[:, :2])\n",
    "\n",
    "    return dst_image, dst_landmarks\n",
    "\n",
    "def draw_lmk_on_image(lmks, img, out_path, color=(0,0,255)):\n",
    "    # draw landmarks on the image\n",
    "    h, w, c = img.shape\n",
    "    for px in lmks[:,:2]:\n",
    "        x, y = int(px[0]), int(px[1])\n",
    "        if 0 <= x < w and 0 <=y < h:\n",
    "            cv2.circle(img, (x, y), 1, color, 1)\n",
    "    cv2.imwrite(out_path, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(img_path)\n",
    "dst_image, dst_landmarks = warp_image_from_lmk(lmks_original_478[frame_id], img, bb_center_shift_y=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(478, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst_landmarks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_warped = (dst_image * 255).astype(np.uint8)\n",
    "draw_lmk_on_image(landmark_478[frame_id], img_warped.copy(), \"test_mediapipe_warp.png\", (255,0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 105, 3])\n",
      "torch.Size([1, 105, 2])\n"
     ]
    }
   ],
   "source": [
    "img_warped = (dst_image * 255).astype(np.uint8)\n",
    "lmk2d_pred = EMICA_rec_to_lmk_2d(flame, frame_id, data_dict)\n",
    "draw_lmk_on_image(lmk2d_pred, img_warped.copy(), \"test_mediapipe_rec_render_yshift0.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_img = np.zeros((224, 224, 3)).astype(np.uint8)\n",
    "draw_lmk_on_image(dst_landmarks, blank_img, \"test_mediapipe_lmk_align_with_rec.png\", (0,0,255))\n",
    "draw_lmk_on_image(lmk2d_pred, blank_img, \"test_mediapipe_lmk_align_with_rec.png\", (255,0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_lmk_on_image(landmark_478_smoothed[0], img_warped.copy(), \"test_mediapipe_smoothed_478.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83, 68, 2)\n"
     ]
    }
   ],
   "source": [
    "# check FAN 68 landmarks\n",
    "lmk_path = 'dataset/mead_25fps/processed/landmarks_aligned/fan/M003/front/angry/level_1/001/landmarks.pkl'\n",
    "with open(lmk_path, 'rb') as f:\n",
    "    lmk_68 = pickle.load(f)\n",
    "print(lmk_68.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmk_68_224 = lmk_68[0].copy() * 224\n",
    "draw_lmk_on_image(lmk_68_224, img_warped.copy(), \"test_fan_aligned.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "X= np.random.rand(100, 1000, 1000).astype('float32')\n",
    "y = np.random.rand(1, 1000, 1000).astype('float32')\n",
    "\n",
    "# Create a new file\n",
    "f = h5py.File('test_h5.hdf5', 'w')\n",
    "f.create_dataset('X_train', data=X)\n",
    "f.create_dataset('y_train', data=y)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train : (100, 1000, 1000)\n",
      "<class 'numpy.ndarray'>\n",
      "shape of y_train : (1, 1000, 1000)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('test_h5.hdf5', \"r\") as f:\n",
    "    for k in f.keys():\n",
    "        print(f\"shape of {k} : {f[k].shape}\")\n",
    "        print(type(f[k][:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 86,  74,  75,  76,  66,  94,  93,  92, 104,  85,  71,  70,  69,\n",
       "        65,  87,  88,  89, 103,  91,  90])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.MediaPipeLandmarkLists import * \n",
    "UPPER_LIP_EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_INDICES[UPPER_LIP_EM[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test audio input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "import librosa\n",
    "import numpy as np  \n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"/local/home/yaqqin/Downloads/013.m4a\" # \"dataset/mead_25fps/original_data/M003/audio/angry/level_1/001.m4a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.221375"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(speech_array) / sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_values = audio_processor(\n",
    "    speech_array, \n",
    "    return_tensors='pt', \n",
    "    padding=\"longest\",\n",
    "    sampling_rate=sampling_rate).input_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 51542])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.221375"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_values.shape[1] / sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.wav2vec import Wav2Vec2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27e846b36474802842b19e513212730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 83, 1024])\n"
     ]
    }
   ],
   "source": [
    "wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "wav2vec.feature_extractor._freeze_parameters()\n",
    "wav2vec.to('cuda')\n",
    " \n",
    "audio_input = audio_values.float().to('cuda')\n",
    "with torch.no_grad():\n",
    "    audio_emb = wav2vec(audio_input, frame_num = 83).last_hidden_state.cpu()\n",
    "    print(audio_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test motion prior checkpoint FLINTV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from munch import Munch, munchify\n",
    "from model.motion_prior import L2lVqVae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = 'pretrained/MotionPrior/models/FLINTv2/checkpoints/model-epoch=0758-val/loss_total=0.113977119327.ckpt'\n",
    "ckpt = torch.load(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['motion_encoder.squasher.0.0.weight', 'motion_encoder.squasher.0.0.bias', 'motion_encoder.squasher.0.2.weight', 'motion_encoder.squasher.0.2.bias', 'motion_encoder.squasher.0.2.running_mean', 'motion_encoder.squasher.0.2.running_var', 'motion_encoder.squasher.0.2.num_batches_tracked', 'motion_encoder.squasher.1.0.weight', 'motion_encoder.squasher.1.0.bias', 'motion_encoder.squasher.1.2.weight', 'motion_encoder.squasher.1.2.bias', 'motion_encoder.squasher.1.2.running_mean', 'motion_encoder.squasher.1.2.running_var', 'motion_encoder.squasher.1.2.num_batches_tracked', 'motion_encoder.squasher.2.0.weight', 'motion_encoder.squasher.2.0.bias', 'motion_encoder.squasher.2.2.weight', 'motion_encoder.squasher.2.2.bias', 'motion_encoder.squasher.2.2.running_mean', 'motion_encoder.squasher.2.2.running_var', 'motion_encoder.squasher.2.2.num_batches_tracked', 'motion_encoder.encoder_transformer.layers.0.self_attn.in_proj_weight', 'motion_encoder.encoder_transformer.layers.0.self_attn.in_proj_bias', 'motion_encoder.encoder_transformer.layers.0.self_attn.out_proj.weight', 'motion_encoder.encoder_transformer.layers.0.self_attn.out_proj.bias', 'motion_encoder.encoder_transformer.layers.0.linear1.weight', 'motion_encoder.encoder_transformer.layers.0.linear1.bias', 'motion_encoder.encoder_transformer.layers.0.linear2.weight', 'motion_encoder.encoder_transformer.layers.0.linear2.bias', 'motion_encoder.encoder_transformer.layers.0.norm1.weight', 'motion_encoder.encoder_transformer.layers.0.norm1.bias', 'motion_encoder.encoder_transformer.layers.0.norm2.weight', 'motion_encoder.encoder_transformer.layers.0.norm2.bias', 'motion_encoder.encoder_linear_embedding.weight', 'motion_encoder.encoder_linear_embedding.bias', 'motion_encoder.mean.weight', 'motion_encoder.mean.bias', 'motion_encoder.logvar.weight', 'motion_encoder.logvar.bias', 'motion_decoder.expander.0.0.weight', 'motion_decoder.expander.0.0.bias', 'motion_decoder.expander.0.2.weight', 'motion_decoder.expander.0.2.bias', 'motion_decoder.expander.0.2.running_mean', 'motion_decoder.expander.0.2.running_var', 'motion_decoder.expander.0.2.num_batches_tracked', 'motion_decoder.expander.1.0.weight', 'motion_decoder.expander.1.0.bias', 'motion_decoder.expander.1.2.weight', 'motion_decoder.expander.1.2.bias', 'motion_decoder.expander.1.2.running_mean', 'motion_decoder.expander.1.2.running_var', 'motion_decoder.expander.1.2.num_batches_tracked', 'motion_decoder.expander.2.0.weight', 'motion_decoder.expander.2.0.bias', 'motion_decoder.expander.2.2.weight', 'motion_decoder.expander.2.2.bias', 'motion_decoder.expander.2.2.running_mean', 'motion_decoder.expander.2.2.running_var', 'motion_decoder.expander.2.2.num_batches_tracked', 'motion_decoder.decoder_transformer.layers.0.self_attn.in_proj_weight', 'motion_decoder.decoder_transformer.layers.0.self_attn.in_proj_bias', 'motion_decoder.decoder_transformer.layers.0.self_attn.out_proj.weight', 'motion_decoder.decoder_transformer.layers.0.self_attn.out_proj.bias', 'motion_decoder.decoder_transformer.layers.0.linear1.weight', 'motion_decoder.decoder_transformer.layers.0.linear1.bias', 'motion_decoder.decoder_transformer.layers.0.linear2.weight', 'motion_decoder.decoder_transformer.layers.0.linear2.bias', 'motion_decoder.decoder_transformer.layers.0.norm1.weight', 'motion_decoder.decoder_transformer.layers.0.norm1.bias', 'motion_decoder.decoder_transformer.layers.0.norm2.weight', 'motion_decoder.decoder_transformer.layers.0.norm2.bias', 'motion_decoder.decoder_linear_embedding.weight', 'motion_decoder.decoder_linear_embedding.bias', 'motion_decoder.cross_smooth_layer.weight', 'motion_decoder.cross_smooth_layer.bias', 'motion_decoder.post_transformer_linear.weight', 'motion_decoder.post_transformer_linear.bias'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['state_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "with h5py.File('dataset/mead_25fps/processed/images/M003/front/disgusted/level_1/002/cropped_frames.hdf5', 'r') as f:\n",
    "    lmk_2d = torch.from_numpy(f['lmk_2d'][:])\n",
    "    img = torch.from_numpy(f['images'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_id = torch.arange(0, lmk_2d.shape[0])\n",
    "n, v = lmk_2d.shape[:2]\n",
    "lmk_mask = torch.ones(n, v)\n",
    "mouth_center = torch.mean(lmk_2d[frame_id, 257:258], dim=1).unsqueeze(1) # (nc, 1, 2)\n",
    "dw, dh = 0.2 + 0.4 * torch.rand(2) # ~uniform(0.2, 0.5)\n",
    "dist_to_center = (lmk_2d[frame_id] - mouth_center).abs() # (nc, V, 2)\n",
    "mask = (dist_to_center[...,0] < dw) & (dist_to_center[...,1] < dh)  # (nc, V)\n",
    "whole_mask = torch.zeros(n, v).bool()\n",
    "whole_mask[frame_id] = mask\n",
    "lmk_mask[whole_mask] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3990), tensor(0.1681))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_lmk_on_image(lmks, img, out_path, color=(0,0,255)):\n",
    "    # draw landmarks on the image\n",
    "    h, w, c = img.shape\n",
    "    for px in lmks[:,:2]:\n",
    "        x, y = int(px[0]), int(px[1])\n",
    "        if 0 <= x < w and 0 <=y < h:\n",
    "            cv2.circle(img, (x, y), 1, color, 1)\n",
    "    cv2.imwrite(out_path, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n",
      "(388, 2)\n"
     ]
    }
   ],
   "source": [
    "lmk_0 = lmk_2d[0]\n",
    "lmk_mask_0 = lmk_mask[0]\n",
    "lmk_0 = lmk_0[lmk_mask_0.bool()]\n",
    "img_0 = (img[0,[2, 1, 0],:,:].permute(1,2,0) * 255.).numpy().astype(np.uint8).copy()\n",
    "print(img_0.shape)\n",
    "lmk_0 = (lmk_0 * 112 + 112).numpy().astype(int)\n",
    "print(lmk_0.shape)\n",
    "draw_lmk_on_image(lmk_0, img_0, 'test_eye_lmk_occlusion.png', color=(0,0,255))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
